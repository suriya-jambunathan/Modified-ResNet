{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81247344-86d2-445c-9259-1740c53e92ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81247344-86d2-445c-9259-1740c53e92ea",
        "outputId": "3e94742b-32be-4db6-ef53-8672f15839bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18776605-ed59-4af8-b6f1-cb84211ad6bc",
      "metadata": {
        "id": "18776605-ed59-4af8-b6f1-cb84211ad6bc"
      },
      "outputs": [],
      "source": [
        "def train_val(model, criterion, optimizer, train_loader, val_loader, device, scheduler = None, use_scheduler = True):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    count = 0\n",
        "    total = 0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        image, label = data\n",
        "        image = image.to(device)\n",
        "        label = label.to(device)\n",
        "    \n",
        "        optimizer.zero_grad()\n",
        "        output = model(image)\n",
        "        loss = criterion(output, label)\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        pred = torch.max(output.data, 1)[1]\n",
        "        cur_correct = (pred == label).sum().item()\n",
        "        cur_loss = loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        \n",
        "        total += label.size(0)\n",
        "        correct += cur_correct\n",
        "        train_loss += cur_loss\n",
        "\n",
        "    train_accuracy = correct/total\n",
        "    train_loss = train_loss/len(train_loader)\n",
        "    \n",
        "    model.eval()\n",
        "    valid_loss = 0\n",
        "    correct = 0\n",
        "    count = 0\n",
        "    total = 0\n",
        "    for i, data in enumerate(val_loader, 0):\n",
        "        image, label = data\n",
        "        image = image.to(device)\n",
        "        label = label.to(device)\n",
        "                \n",
        "        output = model(image)\n",
        "        loss = criterion(output, label)\n",
        "\n",
        "        pred = torch.max(output.data, 1)[1]\n",
        "        cur_correct = (pred == label).sum().item()\n",
        "        cur_loss = loss.item()\n",
        "            \n",
        "        total += label.size(0)\n",
        "        correct += cur_correct\n",
        "        valid_loss += cur_loss\n",
        "\n",
        "    valid_accuracy = correct/total\n",
        "    valid_loss = valid_loss/len(val_loader)\n",
        "    \n",
        "    if use_scheduler:\n",
        "        scheduler.step(valid_accuracy)\n",
        "\n",
        "    return train_loss, train_accuracy, valid_loss, valid_accuracy\n",
        "\n",
        "def test(model, criterion, dataloader, device):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    count = 0\n",
        "    total = 0\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        image, label = data\n",
        "        image = image.to(device)\n",
        "        label = label.to(device)\n",
        "                \n",
        "        output = model(image)\n",
        "        loss = criterion(output, label)\n",
        "\n",
        "        pred = torch.max(output.data, 1)[1]\n",
        "        cur_correct = (pred == label).sum().item()\n",
        "        cur_loss = loss.item()\n",
        "            \n",
        "        total += label.size(0)\n",
        "        correct += cur_correct\n",
        "        test_loss += cur_loss\n",
        "\n",
        "    accuracy = correct/total\n",
        "    test_loss = test_loss/len(dataloader)\n",
        "\n",
        "    return test_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2b02d23-a5d8-437d-b7c3-0b5601e63acb",
      "metadata": {
        "id": "a2b02d23-a5d8-437d-b7c3-0b5601e63acb"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9bc0ea6-4848-48b2-89a5-3a76737c596a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9bc0ea6-4848-48b2-89a5-3a76737c596a",
        "outputId": "4860c0b1-8d9c-4c30-e611-c9a902923cc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Params: 4891338\n",
            "\n"
          ]
        }
      ],
      "source": [
        "class ZigZag_ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ZigZag_ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 128, num_blocks[3], stride=2)\n",
        "        self.layer5 = self._make_layer(block, 64, num_blocks[4], stride=2)\n",
        "        self.layer6 = self._make_layer(block, 128, num_blocks[5], stride=2)\n",
        "        self.layer7 = self._make_layer(block, 256, num_blocks[6], stride=2)\n",
        "        self.linear = nn.Linear(256*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.layer5(out)\n",
        "        out = self.layer6(out)\n",
        "        out = self.layer7(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "zz_model = ZigZag_ResNet(BasicBlock, [2, 2, 2, 2, 2, 1, 1])\n",
        "num_params = sum(p.numel() for p in zz_model.parameters() if p.requires_grad)\n",
        "print(f\"Num Params: {num_params}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94ac267d-981a-4ad6-ba8c-eefb0a40507c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94ac267d-981a-4ad6-ba8c-eefb0a40507c",
        "outputId": "2918363a-0802-4d94-fa1b-ea0a30804c43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:05<00:00, 28657620.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Num Params: 4891338\n",
            "\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "    \n",
        "transform_train = torchvision.transforms.Compose([\n",
        "  torchvision.transforms.RandomCrop(32, padding=4),\n",
        "  torchvision.transforms.RandomHorizontalFlip(),\n",
        "  torchvision.transforms.RandomResizedCrop(32, scale=(0.8, 1.0), ratio=(0.8, 1.2)),\n",
        "  torchvision.transforms.ToTensor(),\n",
        "  torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "transform_test = torchvision.transforms.Compose([\n",
        "                  torchvision.transforms.ToTensor(), \n",
        "                  torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root = './data', train = True, download = True, transform = transform_train)\n",
        "\n",
        "# Split the train data into train and validation sets\n",
        "train_size = int(0.8 * len(trainset))\n",
        "val_size = len(trainset) - train_size\n",
        "trainset, valset = torch.utils.data.random_split(trainset, [train_size, val_size])\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root = './data', train = False, download = True, transform = transform_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size = batch_size, shuffle = True)\n",
        "val_loader   = torch.utils.data.DataLoader(valset, batch_size = batch_size, shuffle = True)\n",
        "test_loader  = torch.utils.data.DataLoader(testset, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "    \n",
        "best_test_acc = 0\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "model = ZigZag_ResNet(BasicBlock, [2, 2, 2, 2, 2, 1, 1])\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Num Params: {num_params}\\n\")\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), 0.01, momentum = 0.8, weight_decay = 0.0005 , nesterov=True)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5, verbose = True)\n",
        "\n",
        "train_losses_ = []\n",
        "train_accuracies_ = []\n",
        "valid_losses_ = []\n",
        "valid_accuracies_ = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13036866-6530-44a0-8bff-0c64c1aaed65",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13036866-6530-44a0-8bff-0c64c1aaed65",
        "outputId": "2e78f705-21e8-441a-8ef4-3233ffeee184"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\tEpoch: 0\n",
            "\tTraining Loss: 3.4509; Training Accuracy: 36.6325%\n",
            "\tValidation Loss: 1.5205; Validation Accuracy: 44.36%\n",
            "\n",
            "\tEpoch: 1\n",
            "\tTraining Loss: 2.7357; Training Accuracy: 50.0875%\n",
            "\tValidation Loss: 1.2269; Validation Accuracy: 56.2%\n",
            "\n",
            "\tEpoch: 2\n",
            "\tTraining Loss: 2.2906; Training Accuracy: 59.1825%\n",
            "\tValidation Loss: 1.0767; Validation Accuracy: 62.1%\n",
            "\n",
            "\tEpoch: 3\n",
            "\tTraining Loss: 1.9235; Training Accuracy: 66.2125%\n",
            "\tValidation Loss: 0.9331; Validation Accuracy: 67.33%\n",
            "\n",
            "\tEpoch: 4\n",
            "\tTraining Loss: 1.6768; Training Accuracy: 70.7875%\n",
            "\tValidation Loss: 0.8811; Validation Accuracy: 69.88%\n",
            "\n",
            "\tEpoch: 5\n",
            "\tTraining Loss: 1.5043; Training Accuracy: 73.7125%\n",
            "\tValidation Loss: 0.734; Validation Accuracy: 74.6%\n",
            "\n",
            "\tEpoch: 6\n",
            "\tTraining Loss: 1.3546; Training Accuracy: 76.7075%\n",
            "\tValidation Loss: 0.6952; Validation Accuracy: 76.35%\n",
            "\n",
            "\tEpoch: 7\n",
            "\tTraining Loss: 1.2548; Training Accuracy: 78.22%\n",
            "\tValidation Loss: 0.6533; Validation Accuracy: 77.74%\n",
            "\n",
            "\tEpoch: 8\n",
            "\tTraining Loss: 1.1762; Training Accuracy: 79.635%\n",
            "\tValidation Loss: 0.5826; Validation Accuracy: 80.09%\n",
            "\n",
            "\tEpoch: 9\n",
            "\tTraining Loss: 1.0855; Training Accuracy: 81.4175%\n",
            "\tValidation Loss: 0.574; Validation Accuracy: 81.15%\n",
            "\n",
            "\tEpoch: 10\n",
            "\tTraining Loss: 1.0218; Training Accuracy: 82.435%\n",
            "\tValidation Loss: 0.5619; Validation Accuracy: 80.68%\n",
            "\n",
            "\tEpoch: 11\n",
            "\tTraining Loss: 0.9651; Training Accuracy: 83.5025%\n",
            "\tValidation Loss: 0.5832; Validation Accuracy: 80.06%\n",
            "\n",
            "\tEpoch: 12\n",
            "\tTraining Loss: 0.9163; Training Accuracy: 84.385%\n",
            "\tValidation Loss: 0.5246; Validation Accuracy: 82.42%\n",
            "\n",
            "\tEpoch: 13\n",
            "\tTraining Loss: 0.8624; Training Accuracy: 85.0375%\n",
            "\tValidation Loss: 0.5326; Validation Accuracy: 82.4%\n",
            "\n",
            "\tEpoch: 14\n",
            "\tTraining Loss: 0.8285; Training Accuracy: 85.585%\n",
            "\tValidation Loss: 0.5299; Validation Accuracy: 82.46%\n",
            "\n",
            "\tEpoch: 15\n",
            "\tTraining Loss: 0.7958; Training Accuracy: 86.3275%\n",
            "\tValidation Loss: 0.4952; Validation Accuracy: 83.48%\n",
            "\n",
            "\tEpoch: 16\n",
            "\tTraining Loss: 0.7571; Training Accuracy: 86.8775%\n",
            "\tValidation Loss: 0.4749; Validation Accuracy: 84.29%\n",
            "\n",
            "\tEpoch: 17\n",
            "\tTraining Loss: 0.7288; Training Accuracy: 87.6075%\n",
            "\tValidation Loss: 0.4761; Validation Accuracy: 83.9%\n",
            "\n",
            "\tEpoch: 18\n",
            "\tTraining Loss: 0.7009; Training Accuracy: 88.0275%\n",
            "\tValidation Loss: 0.4688; Validation Accuracy: 84.21%\n",
            "\n",
            "\tEpoch: 19\n",
            "\tTraining Loss: 0.6719; Training Accuracy: 88.37%\n",
            "\tValidation Loss: 0.4506; Validation Accuracy: 84.94%\n",
            "\n",
            "\tEpoch: 20\n",
            "\tTraining Loss: 0.6507; Training Accuracy: 88.86%\n",
            "\tValidation Loss: 0.4446; Validation Accuracy: 84.99%\n",
            "\n",
            "\tEpoch: 21\n",
            "\tTraining Loss: 0.6336; Training Accuracy: 89.3575%\n",
            "\tValidation Loss: 0.4275; Validation Accuracy: 85.67%\n",
            "\n",
            "\tEpoch: 22\n",
            "\tTraining Loss: 0.6145; Training Accuracy: 89.575%\n",
            "\tValidation Loss: 0.4753; Validation Accuracy: 83.98%\n",
            "\n",
            "\tEpoch: 23\n",
            "\tTraining Loss: 0.584; Training Accuracy: 89.91%\n",
            "\tValidation Loss: 0.4285; Validation Accuracy: 85.92%\n",
            "\n",
            "\tEpoch: 24\n",
            "\tTraining Loss: 0.5642; Training Accuracy: 90.315%\n",
            "\tValidation Loss: 0.4881; Validation Accuracy: 84.08%\n",
            "\n",
            "\tEpoch: 25\n",
            "\tTraining Loss: 0.555; Training Accuracy: 90.455%\n",
            "\tValidation Loss: 0.4338; Validation Accuracy: 85.74%\n",
            "\n",
            "\tEpoch: 26\n",
            "\tTraining Loss: 0.5358; Training Accuracy: 90.955%\n",
            "\tValidation Loss: 0.4059; Validation Accuracy: 86.83%\n",
            "\n",
            "\tEpoch: 27\n",
            "\tTraining Loss: 0.5166; Training Accuracy: 91.155%\n",
            "\tValidation Loss: 0.4139; Validation Accuracy: 86.5%\n",
            "\n",
            "\tEpoch: 28\n",
            "\tTraining Loss: 0.4955; Training Accuracy: 91.4325%\n",
            "\tValidation Loss: 0.4418; Validation Accuracy: 85.36%\n",
            "\n",
            "\tEpoch: 29\n",
            "\tTraining Loss: 0.4765; Training Accuracy: 91.8525%\n",
            "\tValidation Loss: 0.4545; Validation Accuracy: 85.34%\n",
            "\n",
            "\tEpoch: 30\n",
            "\tTraining Loss: 0.4872; Training Accuracy: 91.52%\n",
            "\tValidation Loss: 0.3895; Validation Accuracy: 86.66%\n",
            "\n",
            "\tEpoch: 31\n",
            "\tTraining Loss: 0.4569; Training Accuracy: 92.1975%\n",
            "\tValidation Loss: 0.4505; Validation Accuracy: 85.55%\n",
            "\n",
            "\tEpoch: 32\n",
            "\tTraining Loss: 0.4457; Training Accuracy: 92.1875%\n",
            "\tValidation Loss: 0.4096; Validation Accuracy: 86.97%\n",
            "\n",
            "\tEpoch: 33\n",
            "\tTraining Loss: 0.4344; Training Accuracy: 92.5275%\n",
            "\tValidation Loss: 0.4049; Validation Accuracy: 87.18%\n",
            "\n",
            "\tEpoch: 34\n",
            "\tTraining Loss: 0.4268; Training Accuracy: 92.6325%\n",
            "\tValidation Loss: 0.4282; Validation Accuracy: 86.37%\n",
            "\n",
            "\tEpoch: 35\n",
            "\tTraining Loss: 0.4133; Training Accuracy: 92.8825%\n",
            "\tValidation Loss: 0.4143; Validation Accuracy: 86.48%\n",
            "\n",
            "\tEpoch: 36\n",
            "\tTraining Loss: 0.4001; Training Accuracy: 93.0525%\n",
            "\tValidation Loss: 0.4409; Validation Accuracy: 86.03%\n",
            "\n",
            "\tEpoch: 37\n",
            "\tTraining Loss: 0.4008; Training Accuracy: 93.1025%\n",
            "\tValidation Loss: 0.4061; Validation Accuracy: 87.24%\n",
            "\n",
            "\tEpoch: 38\n",
            "\tTraining Loss: 0.385; Training Accuracy: 93.3175%\n",
            "\tValidation Loss: 0.3876; Validation Accuracy: 87.7%\n",
            "\n",
            "\tEpoch: 39\n",
            "\tTraining Loss: 0.3735; Training Accuracy: 93.5275%\n",
            "\tValidation Loss: 0.3912; Validation Accuracy: 87.84%\n",
            "\n",
            "\tEpoch: 40\n",
            "\tTraining Loss: 0.3646; Training Accuracy: 93.76%\n",
            "\tValidation Loss: 0.4142; Validation Accuracy: 86.95%\n",
            "\n",
            "\tEpoch: 41\n",
            "\tTraining Loss: 0.3552; Training Accuracy: 93.9725%\n",
            "\tValidation Loss: 0.4027; Validation Accuracy: 87.34%\n",
            "\n",
            "\tEpoch: 42\n",
            "\tTraining Loss: 0.3542; Training Accuracy: 94.0075%\n",
            "\tValidation Loss: 0.3731; Validation Accuracy: 88.44%\n",
            "\n",
            "\tEpoch: 43\n",
            "\tTraining Loss: 0.3276; Training Accuracy: 94.295%\n",
            "\tValidation Loss: 0.4108; Validation Accuracy: 87.68%\n",
            "\n",
            "\tEpoch: 44\n",
            "\tTraining Loss: 0.3399; Training Accuracy: 94.1875%\n",
            "\tValidation Loss: 0.3957; Validation Accuracy: 87.64%\n",
            "\n",
            "\tEpoch: 45\n",
            "\tTraining Loss: 0.3269; Training Accuracy: 94.3725%\n",
            "\tValidation Loss: 0.3719; Validation Accuracy: 88.39%\n",
            "\n",
            "\tEpoch: 46\n",
            "\tTraining Loss: 0.3308; Training Accuracy: 94.31%\n",
            "\tValidation Loss: 0.388; Validation Accuracy: 87.8%\n",
            "\n",
            "\tEpoch: 47\n",
            "\tTraining Loss: 0.3066; Training Accuracy: 94.7675%\n",
            "\tValidation Loss: 0.3926; Validation Accuracy: 87.87%\n",
            "\n",
            "\tEpoch: 48\n",
            "Epoch 00049: reducing learning rate of group 0 to 1.0000e-03.\n",
            "\tTraining Loss: 0.3076; Training Accuracy: 94.68%\n",
            "\tValidation Loss: 0.3777; Validation Accuracy: 88.15%\n",
            "\n",
            "\tEpoch: 49\n",
            "\tTraining Loss: 0.1837; Training Accuracy: 96.8975%\n",
            "\tValidation Loss: 0.2966; Validation Accuracy: 90.81%\n",
            "\n",
            "\tEpoch: 50\n",
            "\tTraining Loss: 0.1408; Training Accuracy: 97.715%\n",
            "\tValidation Loss: 0.3082; Validation Accuracy: 91.1%\n",
            "\n",
            "\tEpoch: 51\n",
            "\tTraining Loss: 0.1189; Training Accuracy: 98.115%\n",
            "\tValidation Loss: 0.2956; Validation Accuracy: 91.27%\n",
            "\n",
            "\tEpoch: 52\n",
            "\tTraining Loss: 0.1098; Training Accuracy: 98.1925%\n",
            "\tValidation Loss: 0.3103; Validation Accuracy: 91.05%\n",
            "\n",
            "\tEpoch: 53\n",
            "\tTraining Loss: 0.1034; Training Accuracy: 98.2825%\n",
            "\tValidation Loss: 0.3003; Validation Accuracy: 91.34%\n",
            "\n",
            "\tEpoch: 54\n",
            "\tTraining Loss: 0.096; Training Accuracy: 98.465%\n",
            "\tValidation Loss: 0.3057; Validation Accuracy: 91.15%\n",
            "\n",
            "\tEpoch: 55\n",
            "\tTraining Loss: 0.0918; Training Accuracy: 98.4975%\n",
            "\tValidation Loss: 0.3114; Validation Accuracy: 91.34%\n",
            "\n",
            "\tEpoch: 56\n",
            "\tTraining Loss: 0.0853; Training Accuracy: 98.5875%\n",
            "\tValidation Loss: 0.3129; Validation Accuracy: 91.31%\n",
            "\n",
            "\tEpoch: 57\n",
            "\tTraining Loss: 0.0784; Training Accuracy: 98.7025%\n",
            "\tValidation Loss: 0.3247; Validation Accuracy: 90.88%\n",
            "\n",
            "\tEpoch: 58\n",
            "\tTraining Loss: 0.0826; Training Accuracy: 98.6175%\n",
            "\tValidation Loss: 0.3142; Validation Accuracy: 91.47%\n",
            "\n",
            "\tEpoch: 59\n",
            "\tTraining Loss: 0.0743; Training Accuracy: 98.6925%\n",
            "\tValidation Loss: 0.305; Validation Accuracy: 91.37%\n",
            "\n",
            "\tEpoch: 60\n",
            "\tTraining Loss: 0.0752; Training Accuracy: 98.785%\n",
            "\tValidation Loss: 0.3232; Validation Accuracy: 91.4%\n",
            "\n",
            "\tEpoch: 61\n",
            "\tTraining Loss: 0.0706; Training Accuracy: 98.8375%\n",
            "\tValidation Loss: 0.3085; Validation Accuracy: 91.68%\n",
            "\n",
            "\tEpoch: 62\n",
            "\tTraining Loss: 0.0725; Training Accuracy: 98.775%\n",
            "\tValidation Loss: 0.3174; Validation Accuracy: 91.48%\n",
            "\n",
            "\tEpoch: 63\n",
            "\tTraining Loss: 0.0661; Training Accuracy: 98.9425%\n",
            "\tValidation Loss: 0.3041; Validation Accuracy: 91.6%\n",
            "\n",
            "\tEpoch: 64\n",
            "\tTraining Loss: 0.0673; Training Accuracy: 98.835%\n",
            "\tValidation Loss: 0.3191; Validation Accuracy: 91.4%\n",
            "\n",
            "\tEpoch: 65\n",
            "\tTraining Loss: 0.0605; Training Accuracy: 99.0225%\n",
            "\tValidation Loss: 0.3367; Validation Accuracy: 91.35%\n",
            "\n",
            "\tEpoch: 66\n",
            "\tTraining Loss: 0.0623; Training Accuracy: 98.96%\n",
            "\tValidation Loss: 0.3296; Validation Accuracy: 91.64%\n",
            "\n",
            "\tEpoch: 67\n",
            "Epoch 00068: reducing learning rate of group 0 to 1.0000e-04.\n",
            "\tTraining Loss: 0.0602; Training Accuracy: 99.0025%\n",
            "\tValidation Loss: 0.3201; Validation Accuracy: 91.62%\n",
            "\n",
            "\tEpoch: 68\n",
            "\tTraining Loss: 0.0512; Training Accuracy: 99.18%\n",
            "\tValidation Loss: 0.3322; Validation Accuracy: 91.45%\n",
            "\n",
            "\tEpoch: 69\n",
            "\tTraining Loss: 0.0515; Training Accuracy: 99.17%\n",
            "\tValidation Loss: 0.3177; Validation Accuracy: 92.01%\n",
            "\n",
            "\tEpoch: 70\n",
            "\tTraining Loss: 0.0536; Training Accuracy: 99.165%\n",
            "\tValidation Loss: 0.3139; Validation Accuracy: 91.82%\n",
            "\n",
            "\tEpoch: 71\n",
            "\tTraining Loss: 0.0453; Training Accuracy: 99.3075%\n",
            "\tValidation Loss: 0.3283; Validation Accuracy: 91.53%\n",
            "\n",
            "\tEpoch: 72\n",
            "\tTraining Loss: 0.048; Training Accuracy: 99.265%\n",
            "\tValidation Loss: 0.3173; Validation Accuracy: 91.53%\n",
            "\n",
            "\tEpoch: 73\n",
            "\tTraining Loss: 0.0473; Training Accuracy: 99.1775%\n",
            "\tValidation Loss: 0.3095; Validation Accuracy: 91.82%\n",
            "\n",
            "\tEpoch: 74\n",
            "\tTraining Loss: 0.0493; Training Accuracy: 99.1825%\n",
            "\tValidation Loss: 0.3221; Validation Accuracy: 91.78%\n",
            "\n",
            "\tEpoch: 75\n",
            "Epoch 00076: reducing learning rate of group 0 to 1.0000e-05.\n",
            "\tTraining Loss: 0.0494; Training Accuracy: 99.24%\n",
            "\tValidation Loss: 0.3188; Validation Accuracy: 91.68%\n",
            "\n",
            "\tEpoch: 76\n",
            "\tTraining Loss: 0.0473; Training Accuracy: 99.2425%\n",
            "\tValidation Loss: 0.3281; Validation Accuracy: 91.41%\n",
            "\n",
            "\tEpoch: 77\n",
            "\tTraining Loss: 0.0467; Training Accuracy: 99.2175%\n",
            "\tValidation Loss: 0.3243; Validation Accuracy: 91.78%\n",
            "\n",
            "\tEpoch: 78\n",
            "\tTraining Loss: 0.0457; Training Accuracy: 99.27%\n",
            "\tValidation Loss: 0.3081; Validation Accuracy: 92.07%\n",
            "\n",
            "\tEpoch: 79\n",
            "\tTraining Loss: 0.0456; Training Accuracy: 99.2275%\n",
            "\tValidation Loss: 0.3153; Validation Accuracy: 91.78%\n",
            "\n",
            "\tEpoch: 80\n",
            "\tTraining Loss: 0.0451; Training Accuracy: 99.305%\n",
            "\tValidation Loss: 0.3286; Validation Accuracy: 91.74%\n",
            "\n",
            "\tEpoch: 81\n",
            "\tTraining Loss: 0.0502; Training Accuracy: 99.2%\n",
            "\tValidation Loss: 0.32; Validation Accuracy: 91.54%\n",
            "\n",
            "\tEpoch: 82\n",
            "\tTraining Loss: 0.0492; Training Accuracy: 99.185%\n",
            "\tValidation Loss: 0.3197; Validation Accuracy: 91.74%\n",
            "\n",
            "\tEpoch: 83\n",
            "\tTraining Loss: 0.0454; Training Accuracy: 99.235%\n",
            "\tValidation Loss: 0.3196; Validation Accuracy: 91.59%\n",
            "\n",
            "\tEpoch: 84\n",
            "Epoch 00085: reducing learning rate of group 0 to 1.0000e-06.\n",
            "\tTraining Loss: 0.0474; Training Accuracy: 99.2225%\n",
            "\tValidation Loss: 0.3283; Validation Accuracy: 91.46%\n",
            "\n",
            "\tEpoch: 85\n",
            "\tTraining Loss: 0.0445; Training Accuracy: 99.3125%\n",
            "\tValidation Loss: 0.324; Validation Accuracy: 91.66%\n",
            "\n",
            "\tEpoch: 86\n",
            "\tTraining Loss: 0.0427; Training Accuracy: 99.3475%\n",
            "\tValidation Loss: 0.3166; Validation Accuracy: 91.7%\n",
            "\n",
            "\tEpoch: 87\n",
            "\tTraining Loss: 0.0455; Training Accuracy: 99.2775%\n",
            "\tValidation Loss: 0.3239; Validation Accuracy: 91.63%\n",
            "\n",
            "\tEpoch: 88\n",
            "\tTraining Loss: 0.0494; Training Accuracy: 99.1825%\n",
            "\tValidation Loss: 0.3141; Validation Accuracy: 91.76%\n",
            "\n",
            "\tEpoch: 89\n",
            "\tTraining Loss: 0.0467; Training Accuracy: 99.25%\n",
            "\tValidation Loss: 0.3181; Validation Accuracy: 91.83%\n",
            "\n",
            "\tEpoch: 90\n",
            "Epoch 00091: reducing learning rate of group 0 to 1.0000e-07.\n",
            "\tTraining Loss: 0.0453; Training Accuracy: 99.2675%\n",
            "\tValidation Loss: 0.3318; Validation Accuracy: 91.48%\n",
            "\n",
            "\tEpoch: 91\n",
            "\tTraining Loss: 0.046; Training Accuracy: 99.2675%\n",
            "\tValidation Loss: 0.3307; Validation Accuracy: 91.45%\n",
            "\n",
            "\tEpoch: 92\n",
            "\tTraining Loss: 0.0467; Training Accuracy: 99.26%\n",
            "\tValidation Loss: 0.3196; Validation Accuracy: 91.72%\n",
            "\n",
            "\tEpoch: 93\n",
            "\tTraining Loss: 0.0437; Training Accuracy: 99.3025%\n",
            "\tValidation Loss: 0.3287; Validation Accuracy: 91.49%\n",
            "\n",
            "\tEpoch: 94\n",
            "\tTraining Loss: 0.0492; Training Accuracy: 99.2225%\n",
            "\tValidation Loss: 0.3193; Validation Accuracy: 91.79%\n",
            "\n",
            "\tEpoch: 95\n",
            "\tTraining Loss: 0.0481; Training Accuracy: 99.22%\n",
            "\tValidation Loss: 0.3262; Validation Accuracy: 91.78%\n",
            "\n",
            "\tEpoch: 96\n",
            "Epoch 00097: reducing learning rate of group 0 to 1.0000e-08.\n",
            "\tTraining Loss: 0.0467; Training Accuracy: 99.2225%\n",
            "\tValidation Loss: 0.3241; Validation Accuracy: 91.87%\n",
            "\n",
            "\tEpoch: 97\n",
            "\tTraining Loss: 0.0458; Training Accuracy: 99.2525%\n",
            "\tValidation Loss: 0.325; Validation Accuracy: 91.8%\n",
            "\n",
            "\tEpoch: 98\n",
            "\tTraining Loss: 0.0499; Training Accuracy: 99.2%\n",
            "\tValidation Loss: 0.3206; Validation Accuracy: 91.76%\n",
            "\n",
            "\tEpoch: 99\n",
            "\tTraining Loss: 0.047; Training Accuracy: 99.26%\n",
            "\tValidation Loss: 0.313; Validation Accuracy: 91.93%\n",
            "\n",
            "\tTesting Loss: 0.3024; Testing Accuracy: 92.07%\n"
          ]
        }
      ],
      "source": [
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\n\\tEpoch: {epoch}\")\n",
        "\n",
        "    train_loss, train_accuracy, val_loss, val_accuracy = train_val(model, criterion, optimizer, \n",
        "                                                                train_loader, val_loader, device,\n",
        "                                                                scheduler = scheduler, use_scheduler = True)\n",
        "    train_losses_.append(train_loss)\n",
        "    train_accuracies_.append(train_accuracy)\n",
        "    valid_losses_.append(val_loss)\n",
        "    valid_accuracies_.append(val_accuracy)\n",
        "    print(f\"\\tTraining Loss: {round(train_loss, 4)}; Training Accuracy: {round(train_accuracy*100, 4)}%\")\n",
        "    print(f\"\\tValidation Loss: {round(val_loss, 4)}; Validation Accuracy: {round(val_accuracy*100, 4)}%\")\n",
        "\n",
        "test_loss, test_accuracy = test(model, criterion, test_loader, device)\n",
        "print(f\"\\n\\tTesting Loss: {round(test_loss, 4)}; Testing Accuracy: {round(test_accuracy*100, 4)}%\")\n",
        "\n",
        "if test_accuracy > best_test_acc:\n",
        "    best_test_acc = test_accuracy\n",
        "\n",
        "    torch.save(model.state_dict(), 'zigzag_resnet_tuned.pth')\n",
        "\n",
        "    metrics_dict = {'train_loss': train_losses_, 'train_accuracy': train_accuracies_, \n",
        "                  'valid_loss': valid_losses_, 'valid_accuracy': valid_accuracies_,\n",
        "                  'test_loss': test_loss, 'test_accuracy': test_accuracy}"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}